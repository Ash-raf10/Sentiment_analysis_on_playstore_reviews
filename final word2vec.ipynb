{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv dataset then edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"googleplaystore_user_reviews.csv\", na_values=\"nan\")\n",
    "df = df.dropna(subset=['App','Translated_Review','Sentiment'], how='any')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Positive'],'1')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Negative'],'0')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Neutral'],'1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reviews are categorized as lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37427\n"
     ]
    }
   ],
   "source": [
    "review_lines = list()\n",
    "lines = df['Translated_Review'].values.tolist()\n",
    "print (len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization and removing punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in lines :\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table =str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "len(review_lines)\n",
    "#print(review_lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word: 21481\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=review_lines,size=100,window = 5,workers =4,min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "print('total word: %d' %len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'r.txt'\n",
    "model.wv.save_word2vec_format(filename,binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding as a directory of words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('','r.txt'),encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting the word embedding into tokenized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 21481 unique tokens \n",
      "Shape of review  (37427, 100)\n",
      "shape of senti (37427,)\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(review_lines)\n",
    "sequences = tk.texts_to_sequences(review_lines)\n",
    "word_index = tk.word_index\n",
    "print(\"found %s unique tokens \" % len(word_index))\n",
    "review_pad = pad_sequences(sequences,maxlen=100)\n",
    "sentiment = df['Sentiment'].values\n",
    "print('Shape of review ', review_pad.shape)\n",
    "print('shape of senti' , sentiment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map embeddings from the loaded word2vec model for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21482\n"
     ]
    }
   ],
   "source": [
    " num_words = len(word_index) + 1\n",
    "embedd = np.zeros((num_words,100))\n",
    "\n",
    "for word , i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedd_vec = embeddings_index.get(word)\n",
    "    if embedd_vec is not None:\n",
    "        embedd[i] = embedd_vec     \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding matrix as input to the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import constant\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer =  Embedding(num_words,100,embeddings_initializer= constant(embedd),input_length=100,trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the sentiment classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_pad  (29942, 100)\n",
      "shape of y_train  (29942,)\n",
      "shape of X_test_pad  (7485, 100)\n",
      "shape of y_train  (7485,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation = int (VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation]\n",
    "y_train = sentiment[:-num_validation]\n",
    "X_test_pad = review_pad[-num_validation:]\n",
    "y_test = sentiment[-num_validation:]\n",
    "\n",
    "\n",
    "\n",
    "print('shape of X_train_pad ', X_train_pad.shape)\n",
    "print('shape of y_train ', y_train.shape)\n",
    "\n",
    "print('shape of X_test_pad ', X_test_pad.shape)\n",
    "print('shape of y_train ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the classification model on train and validation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 29942 samples, validate on 7485 samples\n",
      "Epoch 1/10\n",
      " - 30s - loss: 0.4379 - acc: 0.7939 - val_loss: 0.3727 - val_acc: 0.8269\n",
      "Epoch 2/10\n",
      " - 30s - loss: 0.3781 - acc: 0.8257 - val_loss: 0.3555 - val_acc: 0.8362\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.3603 - acc: 0.8331 - val_loss: 0.3407 - val_acc: 0.8456\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.3473 - acc: 0.8427 - val_loss: 0.3310 - val_acc: 0.8488\n",
      "Epoch 5/10\n",
      " - 32s - loss: 0.3410 - acc: 0.8453 - val_loss: 0.3381 - val_acc: 0.8425\n",
      "Epoch 6/10\n",
      " - 32s - loss: 0.3348 - acc: 0.8494 - val_loss: 0.3196 - val_acc: 0.8572\n",
      "Epoch 7/10\n",
      " - 33s - loss: 0.3291 - acc: 0.8527 - val_loss: 0.3165 - val_acc: 0.8573\n",
      "Epoch 8/10\n",
      " - 33s - loss: 0.3247 - acc: 0.8551 - val_loss: 0.3140 - val_acc: 0.8619\n",
      "Epoch 9/10\n",
      " - 34s - loss: 0.3221 - acc: 0.8579 - val_loss: 0.3080 - val_acc: 0.8639\n",
      "Epoch 10/10\n",
      " - 35s - loss: 0.3174 - acc: 0.8601 - val_loss: 0.3077 - val_acc: 0.8637\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=64,epochs=10,validation_data= (X_test_pad,y_test),verbose=2)\n",
    "scores = model.evaluate(X_test_pad, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# printing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9428793 ],\n",
       "       [0.22566655],\n",
       "       [0.04121037]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample1=\"just loving it\"\n",
    "test_sample2=\"I hate using this button,please fix it\"\n",
    "test_sample3=\"this app is bad \"\n",
    "\n",
    "\n",
    "test_samples = [test_sample1,test_sample2,test_sample3]\n",
    "test_samples_tokens = tk.texts_to_sequences(test_samples)\n",
    "\n",
    "pad =pad_sequences(test_samples_tokens,maxlen=100)\n",
    "\n",
    "model.predict(x =pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
